# Memory - 2026-01-30

## Cron Optimization - Option C Implementation (02:05 UTC / 9:05 PM EST Jan 29)

**Context:** Yesterday's docs deep-dive revealed I was using cron/heartbeat wrong. Discussed three approaches with Shirin.

**Decision:** Option C (Hybrid) - batch flexible checks in heartbeat, keep scheduled deliverables as cron.

**Changes made:**
1. **Removed cron jobs (3 total):**
   - `gmail-scan` (1 AM) - Removed entirely per Shirin's request
   - `daily-security-report` (10 AM) - Moved to heartbeat (flexible timing)
   - `system-cleanup-logging` (2:30 AM) - Moved to heartbeat (flexible timing)

2. **Added cron job:**
   - `improvement-analysis` (4:30 AM EST) - Generates daily improvement proposals before morning brief

3. **Rewrote HEARTBEAT.md:**
   - Clear "when to act vs stay silent" section
   - Specific triggers for each check (no vague "every 3-4 hours")
   - Security monitoring: Check logs, alert if >50 SSH failures or unusual activity
   - System maintenance: Check disk/memory, auto-cleanup when safe
   - Overnight autonomous work: 1-5 AM window, pick task, work 30-90 min
   - Memory review: Use CLI tools, merge fragments
   - Self-review: Document mistakes, track failure modes

4. **Updated CRON.json** to reflect current state (13 jobs remaining)

**Result:**
- 3 fewer cron jobs (16 → 13)
- More flexible security/system checks (runs when heartbeat polls vs fixed time)
- Clearer heartbeat logic (no more "every 3-4 hours" ambiguity)
- Improvement analysis now feeds morning brief automatically

**Git commit:** d87229c

**Token savings:** ~2-3 API calls/day (batching security + system checks in one heartbeat turn)

**Next in fixing things I was doing wrong:**
- [x] ~~Memory search discipline~~ ✅ DONE
- [ ] Memory writing automation  
- [ ] Model fallback config (when API keys ready)
- [ ] Session freshness (suggest /new more)

---

#cron-optimization #heartbeat #token-efficiency

## Memory Search Discipline - Implemented (02:15 UTC / 9:15 PM EST Jan 29)

**Problem:** Not using `memory_search` before answering questions about prior work, decisions, dates, or re-reading files. Wasting tokens.

**Solution implemented:**

1. **Added mandatory search rules to AGENTS.md:**
   - Clear triggers: prior work, decisions, dates, people, preferences, todos
   - Search-first pattern: `memory_search` → `memory_get` → answer
   - Honesty rule: never pretend to search if you didn't
   - Indexing caveat: async, recent changes might not appear immediately

2. **Created examples doc:** `/root/clawd/docs/memory-search-examples.md`
   - Good vs bad response patterns
   - When to search, when not to search
   - Token efficiency comparison (92% savings for targeted recall)
   - Anti-patterns to avoid

3. **Tested current state:**
   - Memory search is configured: `sessionMemory: true`, sources: memory + sessions
   - Currently returning empty (async indexing not caught up yet)
   - Pattern still valid: search first, fall back to file read if empty

**Token savings potential:**
- Without search: ~3300 tokens (read MEMORY.md + daily files)
- With search: ~250 tokens (query + targeted section)
- **Savings: 92% for targeted recall**

**Next:** Memory writing automation (write + commit immediately after significant events)

**Git commit:** 4255551

---

#memory-search #token-efficiency #discipline

## DeepSeek API Configuration - Complete (02:13 UTC / 9:13 PM EST Jan 29)

**API key stored:** `/root/.env` (secure, not in git, 600 permissions)

**Config applied:**
1. Added `deepseek:default` auth profile
2. Created custom provider `deepseek` with OpenAI-compatible API
3. Models registered:
   - `deepseek-chat` (64K context, 8K max tokens)
   - `deepseek-reasoner` (R1 model, 64K context)
4. Alias created: `deepseek` → `deepseek/deepseek-chat`
5. Fallback chain: `claude-sonnet-4-5` → `deepseek/deepseek-chat`

**Tested:**
- API key validated ✅
- Models visible via API ✅
- Gateway restarted successfully ✅

**How fallback works:**
- Primary: Claude Sonnet 4.5
- On rate limit/auth failure/billing issues: Auto-switches to DeepSeek
- Automatic cooldown + exponential backoff
- Cooldown state: `~/.clawdbot/agents/<agentId>/agent/auth-profiles.json`

**Cost comparison:**
- Claude: ~$3 / 1M input, ~$15 / 1M output
- DeepSeek: ~$0.14 / 1M input, ~$0.28 / 1M output
- **Savings: ~95% cheaper on fallback**

**Next in "things I was doing wrong":**
- [x] ~~Model fallback config~~ ✅ DONE
- [ ] Memory writing automation

---

#deepseek #model-fallback #cost-optimization
